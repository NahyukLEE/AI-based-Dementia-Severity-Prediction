{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "upper-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as tr\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "\n",
    "import visdom\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greenhouse-superintendent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "## terminal command : python -m visdom.server\n",
    "vis = visdom.Visdom()\n",
    "vis2 = visdom.Visdom()\n",
    "plot = vis.line(Y=torch.tensor([0]), X=torch.tensor([0]))\n",
    "plot2= vis2.line(Y=torch.tensor([0]),X=torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informed-structure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faced-argument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 250\n",
      "    Root location: ./test4_preprocessed\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "transf = tr.Compose([tr.Resize((224, 224)),\n",
    "                     tr.ToTensor(),\n",
    "                     tr.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # 128x128 이미지 크기 변환 후 텐서 제작\n",
    "image_datasets = torchvision.datasets.ImageFolder(root='./test4_preprocessed', transform=transf) # 4번 검사 데이터 데이터 로딩\n",
    "print(image_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "residential-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Deform&PSV', 'Normal']\n"
     ]
    }
   ],
   "source": [
    "class_names = image_datasets.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spare-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(image_datasets))\n",
    "test_size = len(image_datasets) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(image_datasets, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "validation_loader = DataLoader(test_dataset, batch_size = 8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "separate-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    #Imshow for Tensor#\n",
    "    inp = inp.numpy().transpose((1,2,0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp,0,1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "material-allen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "inputs, classes = next(iter(train_loader))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brave-eclipse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out,title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "compound-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quarterly-wrestling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dimensional-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dependent-perth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "competitive-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수, 최적화 함수 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "affected-douglas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train loss: 0.672 | Train accuracy: 58.86%\n",
      "[2] Train loss: 0.567 | Train accuracy: 70.86%\n",
      "[3] Train loss: 0.374 | Train accuracy: 85.14%\n",
      "[4] Train loss: 0.301 | Train accuracy: 88.57%\n",
      "[5] Train loss: 0.190 | Train accuracy: 92.00%\n",
      "[6] Train loss: 0.175 | Train accuracy: 94.29%\n",
      "[7] Train loss: 0.420 | Train accuracy: 89.71%\n",
      "[8] Train loss: 0.395 | Train accuracy: 85.14%\n",
      "[9] Train loss: 0.260 | Train accuracy: 88.00%\n",
      "[10] Train loss: 0.308 | Train accuracy: 92.00%\n",
      "[11] Train loss: 0.131 | Train accuracy: 97.14%\n",
      "[12] Train loss: 0.071 | Train accuracy: 98.29%\n",
      "[13] Train loss: 0.078 | Train accuracy: 97.71%\n",
      "[14] Train loss: 0.027 | Train accuracy: 99.43%\n",
      "[15] Train loss: 0.019 | Train accuracy: 99.43%\n",
      "[16] Train loss: 0.008 | Train accuracy: 100.00%\n",
      "[17] Train loss: 0.003 | Train accuracy: 100.00%\n",
      "[18] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "[19] Train loss: 0.003 | Train accuracy: 100.00%\n",
      "[20] Train loss: 0.005 | Train accuracy: 100.00%\n",
      "[21] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "[22] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "[23] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "[24] Train loss: 0.405 | Train accuracy: 85.14%\n",
      "[25] Train loss: 0.309 | Train accuracy: 88.57%\n",
      "[26] Train loss: 0.217 | Train accuracy: 93.14%\n",
      "[27] Train loss: 0.065 | Train accuracy: 98.29%\n",
      "[28] Train loss: 0.152 | Train accuracy: 96.00%\n",
      "[29] Train loss: 0.331 | Train accuracy: 88.00%\n",
      "[30] Train loss: 0.141 | Train accuracy: 94.86%\n",
      "[31] Train loss: 0.031 | Train accuracy: 99.43%\n",
      "[32] Train loss: 0.014 | Train accuracy: 99.43%\n",
      "[33] Train loss: 0.011 | Train accuracy: 99.43%\n",
      "[34] Train loss: 0.007 | Train accuracy: 100.00%\n",
      "[35] Train loss: 0.005 | Train accuracy: 100.00%\n",
      "[36] Train loss: 0.003 | Train accuracy: 100.00%\n",
      "[37] Train loss: 0.007 | Train accuracy: 100.00%\n",
      "[38] Train loss: 0.315 | Train accuracy: 86.29%\n",
      "[39] Train loss: 0.145 | Train accuracy: 95.43%\n",
      "[40] Train loss: 0.056 | Train accuracy: 98.29%\n",
      "[41] Train loss: 0.235 | Train accuracy: 90.86%\n",
      "[42] Train loss: 0.103 | Train accuracy: 97.14%\n",
      "[43] Train loss: 0.032 | Train accuracy: 99.43%\n",
      "[44] Train loss: 0.024 | Train accuracy: 99.43%\n",
      "[45] Train loss: 0.024 | Train accuracy: 99.43%\n",
      "[46] Train loss: 0.019 | Train accuracy: 99.43%\n",
      "[47] Train loss: 0.014 | Train accuracy: 99.43%\n",
      "[48] Train loss: 0.013 | Train accuracy: 99.43%\n",
      "[49] Train loss: 0.021 | Train accuracy: 98.86%\n",
      "[50] Train loss: 0.079 | Train accuracy: 97.71%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for data in train_loader:\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    cost = running_loss / len(train_loader)\n",
    "    \n",
    "    print('[%d] Train loss: %.3f | Train accuracy: %.2f%%' %(epoch + 1, cost, 100.*correct/len(train_loader.dataset))) \n",
    "    \n",
    "    #test loss 값을 Y축, 전달받은 파라미터 epoch를 X 값으로 \n",
    "    vis.line(Y=[cost], X=np.array([epoch]),win=plot,update='append')\n",
    "\n",
    "    # accuracy를 구하는 수식을 Y값으로 epoch를 X값으로 \n",
    "    vis2.line(Y=[100.*correct/len(train_loader.dataset)], X=np.array([epoch]),win=plot2,update='append')\n",
    "    \n",
    "torch.save(model.state_dict(), './models/cropped_test4_vgg16_pretrained.pth')      \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "handed-generation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.vgg16(pretrained=False)\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('./models/cropped_test4_vgg16_pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sunset-serbia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 81.33 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "neither-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 81.33 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    real_array = []\n",
    "    pred_array = []\n",
    "    for data in validation_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        real_array.append(labels.tolist())\n",
    "        pred_array.append(predicted.tolist())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    real_array = sum(real_array, []) #1차원 리스트로\n",
    "    pred_array = sum(pred_array, []) #1차원 리스트로\n",
    "print('Accuracy of the network on the test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "peripheral-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Deform&PSV       0.83      0.83      0.83        41\n",
      "      Normal       0.79      0.79      0.79        34\n",
      "\n",
      "    accuracy                           0.81        75\n",
      "   macro avg       0.81      0.81      0.81        75\n",
      "weighted avg       0.81      0.81      0.81        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(real_array, pred_array, target_names=['Deform&PSV','Normal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-trinidad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 4)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수, 최적화 함수 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for data in train_loader:\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    cost = running_loss / len(train_loader)\n",
    "    \n",
    "    print('[%d] Train loss: %.3f | Train accuracy: %.2f%%' %(epoch + 1, cost, 100.*correct/len(train_loader.dataset))) \n",
    "    \n",
    "    #test loss 값을 Y축, 전달받은 파라미터 epoch를 X 값으로 \n",
    "    vis.line(Y=[cost], X=np.array([epoch]),win=plot,update='append')\n",
    "\n",
    "    # accuracy를 구하는 수식을 Y값으로 epoch를 X값으로 \n",
    "    vis2.line(Y=[100.*correct/len(train_loader.dataset)], X=np.array([epoch]),win=plot2,update='append')\n",
    "    \n",
    "torch.save(model.state_dict(), './models/test4_vgg16.pth')      \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(pretrained=False)\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 4)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('./models/test4_vgg16.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-superintendent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
