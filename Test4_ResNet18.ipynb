{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conservative-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as tr\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "\n",
    "import visdom\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-formation",
   "metadata": {},
   "source": [
    "### Vis 실시간 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pleasant-desert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "## terminal command : python -m visdom.server\n",
    "vis = visdom.Visdom()\n",
    "vis2 = visdom.Visdom()\n",
    "plot = vis.line(Y=torch.tensor([0]), X=torch.tensor([0]))\n",
    "plot2= vis2.line(Y=torch.tensor([0]),X=torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-router",
   "metadata": {},
   "source": [
    "### CUDA Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "falling-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-mattress",
   "metadata": {},
   "source": [
    "### Transfer 설정 및 이미지 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "original-storm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 666\n",
      "    Root location: ./CNN\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "transf = tr.Compose([tr.Resize((128, 128)),\n",
    "                     tr.ToTensor(),\n",
    "                     tr.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # 128x128 이미지 크기 변환 후 텐서 제작\n",
    "image_datasets = torchvision.datasets.ImageFolder(root='./CNN', transform=transf) # 4번 검사 데이터 데이터 로딩\n",
    "print(image_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minute-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Deform&PSV', 'Deformed', 'Normal', 'PSV']\n"
     ]
    }
   ],
   "source": [
    "class_names = image_datasets.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mineral-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.6 * len(image_datasets))\n",
    "test_size = len(image_datasets) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(image_datasets, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "validation_loader = DataLoader(test_dataset, batch_size = 8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fifth-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    #Imshow for Tensor#\n",
    "    inp = inp.numpy().transpose((1,2,0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp,0,1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civic-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs, classes = next(iter(train_loader))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afraid-workplace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out,title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spatial-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blond-restriction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "split-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model.fc.in_features # fc의 입력 노드 수를 산출한다. 512개\n",
    "model.fc = nn.Linear(num_ftrs, 4) # fc를 nn.Linear(num_ftrs, 10)로 대체한다.\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ranging-necessity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accessible-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수, 최적화 함수 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "normal-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train loss: 0.661 | Train accuracy: 78.70%\n",
      "[2] Train loss: 0.309 | Train accuracy: 91.48%\n",
      "[3] Train loss: 0.167 | Train accuracy: 95.24%\n",
      "[4] Train loss: 0.105 | Train accuracy: 96.99%\n",
      "[5] Train loss: 0.084 | Train accuracy: 97.49%\n",
      "[6] Train loss: 0.059 | Train accuracy: 98.75%\n",
      "[7] Train loss: 0.079 | Train accuracy: 96.99%\n",
      "[8] Train loss: 0.065 | Train accuracy: 97.49%\n",
      "[9] Train loss: 0.063 | Train accuracy: 98.25%\n",
      "[10] Train loss: 0.047 | Train accuracy: 99.00%\n",
      "[11] Train loss: 0.091 | Train accuracy: 98.50%\n",
      "[12] Train loss: 0.061 | Train accuracy: 97.99%\n",
      "[13] Train loss: 0.144 | Train accuracy: 93.98%\n",
      "[14] Train loss: 0.193 | Train accuracy: 93.98%\n",
      "[15] Train loss: 0.093 | Train accuracy: 95.74%\n",
      "[16] Train loss: 0.084 | Train accuracy: 97.74%\n",
      "[17] Train loss: 0.018 | Train accuracy: 99.50%\n",
      "[18] Train loss: 0.015 | Train accuracy: 99.75%\n",
      "[19] Train loss: 0.014 | Train accuracy: 99.50%\n",
      "[20] Train loss: 0.023 | Train accuracy: 99.25%\n",
      "[21] Train loss: 0.010 | Train accuracy: 99.75%\n",
      "[22] Train loss: 0.024 | Train accuracy: 98.75%\n",
      "[23] Train loss: 0.007 | Train accuracy: 99.75%\n",
      "[24] Train loss: 0.037 | Train accuracy: 99.00%\n",
      "[25] Train loss: 0.066 | Train accuracy: 97.99%\n",
      "[26] Train loss: 0.095 | Train accuracy: 95.99%\n",
      "[27] Train loss: 0.105 | Train accuracy: 96.99%\n",
      "[28] Train loss: 0.108 | Train accuracy: 96.99%\n",
      "[29] Train loss: 0.050 | Train accuracy: 98.75%\n",
      "[30] Train loss: 0.038 | Train accuracy: 98.75%\n",
      "[31] Train loss: 0.050 | Train accuracy: 99.00%\n",
      "[32] Train loss: 0.039 | Train accuracy: 99.25%\n",
      "[33] Train loss: 0.005 | Train accuracy: 100.00%\n",
      "[34] Train loss: 0.003 | Train accuracy: 100.00%\n",
      "[35] Train loss: 0.008 | Train accuracy: 99.75%\n",
      "[36] Train loss: 0.037 | Train accuracy: 99.00%\n",
      "[37] Train loss: 0.022 | Train accuracy: 99.75%\n",
      "[38] Train loss: 0.077 | Train accuracy: 98.25%\n",
      "[39] Train loss: 0.064 | Train accuracy: 98.25%\n",
      "[40] Train loss: 0.124 | Train accuracy: 94.99%\n",
      "[41] Train loss: 0.053 | Train accuracy: 98.75%\n",
      "[42] Train loss: 0.041 | Train accuracy: 99.00%\n",
      "[43] Train loss: 0.020 | Train accuracy: 99.50%\n",
      "[44] Train loss: 0.013 | Train accuracy: 99.75%\n",
      "[45] Train loss: 0.004 | Train accuracy: 100.00%\n",
      "[46] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "[47] Train loss: 0.025 | Train accuracy: 99.75%\n",
      "[48] Train loss: 0.006 | Train accuracy: 100.00%\n",
      "[49] Train loss: 0.025 | Train accuracy: 99.50%\n",
      "[50] Train loss: 0.018 | Train accuracy: 99.25%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for data in train_loader:\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    cost = running_loss / len(train_loader)\n",
    "    \n",
    "    print('[%d] Train loss: %.3f | Train accuracy: %.2f%%' %(epoch + 1, cost, 100.*correct/len(train_loader.dataset))) \n",
    "    \n",
    "    #test loss 값을 Y축, 전달받은 파라미터 epoch를 X 값으로 \n",
    "    vis.line(Y=[cost], X=np.array([epoch]),win=plot,update='append')\n",
    "\n",
    "    # accuracy를 구하는 수식을 Y값으로 epoch를 X값으로 \n",
    "    vis2.line(Y=[100.*correct/len(train_loader.dataset)], X=np.array([epoch]),win=plot2,update='append')\n",
    "    \n",
    "torch.save(model.state_dict(), './models/test4_resnet18_pretrained.pth')      \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "identical-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 4) \n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('./models/test4_resnet18_pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sexual-medication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 86.89 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "quick-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features # fc의 입력 노드 수를 산출한다. 512개\n",
    "model.fc = nn.Linear(num_ftrs, 4) # fc를 nn.Linear(num_ftrs, 10)로 대체한다.\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "powerful-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수, 최적화 함수 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "educational-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train loss: 0.781 | Train accuracy: 75.69%\n",
      "[2] Train loss: 0.519 | Train accuracy: 82.96%\n",
      "[3] Train loss: 0.340 | Train accuracy: 89.47%\n",
      "[4] Train loss: 0.242 | Train accuracy: 91.23%\n",
      "[5] Train loss: 0.126 | Train accuracy: 96.24%\n",
      "[6] Train loss: 0.064 | Train accuracy: 97.99%\n",
      "[7] Train loss: 0.134 | Train accuracy: 96.24%\n",
      "[8] Train loss: 0.144 | Train accuracy: 95.49%\n",
      "[9] Train loss: 0.068 | Train accuracy: 97.74%\n",
      "[10] Train loss: 0.064 | Train accuracy: 98.75%\n",
      "[11] Train loss: 0.059 | Train accuracy: 97.99%\n",
      "[12] Train loss: 0.059 | Train accuracy: 98.25%\n",
      "[13] Train loss: 0.090 | Train accuracy: 96.74%\n",
      "[14] Train loss: 0.073 | Train accuracy: 97.99%\n",
      "[15] Train loss: 0.057 | Train accuracy: 98.75%\n",
      "[16] Train loss: 0.065 | Train accuracy: 97.99%\n",
      "[17] Train loss: 0.028 | Train accuracy: 99.00%\n",
      "[18] Train loss: 0.087 | Train accuracy: 97.24%\n",
      "[19] Train loss: 0.064 | Train accuracy: 97.49%\n",
      "[20] Train loss: 0.071 | Train accuracy: 97.49%\n",
      "[21] Train loss: 0.067 | Train accuracy: 98.25%\n",
      "[22] Train loss: 0.012 | Train accuracy: 100.00%\n",
      "[23] Train loss: 0.005 | Train accuracy: 100.00%\n",
      "[24] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "[25] Train loss: 0.001 | Train accuracy: 100.00%\n",
      "[26] Train loss: 0.006 | Train accuracy: 100.00%\n",
      "[27] Train loss: 0.053 | Train accuracy: 98.25%\n",
      "[28] Train loss: 0.125 | Train accuracy: 96.49%\n",
      "[29] Train loss: 0.078 | Train accuracy: 97.74%\n",
      "[30] Train loss: 0.044 | Train accuracy: 99.00%\n",
      "[31] Train loss: 0.024 | Train accuracy: 99.25%\n",
      "[32] Train loss: 0.019 | Train accuracy: 99.25%\n",
      "[33] Train loss: 0.014 | Train accuracy: 99.25%\n",
      "[34] Train loss: 0.006 | Train accuracy: 99.75%\n",
      "[35] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "[36] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "[37] Train loss: 0.001 | Train accuracy: 100.00%\n",
      "[38] Train loss: 0.045 | Train accuracy: 99.00%\n",
      "[39] Train loss: 0.140 | Train accuracy: 94.99%\n",
      "[40] Train loss: 0.078 | Train accuracy: 98.25%\n",
      "[41] Train loss: 0.011 | Train accuracy: 100.00%\n",
      "[42] Train loss: 0.020 | Train accuracy: 99.75%\n",
      "[43] Train loss: 0.011 | Train accuracy: 99.75%\n",
      "[44] Train loss: 0.007 | Train accuracy: 99.75%\n",
      "[45] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "[46] Train loss: 0.031 | Train accuracy: 99.00%\n",
      "[47] Train loss: 0.049 | Train accuracy: 98.50%\n",
      "[48] Train loss: 0.026 | Train accuracy: 99.25%\n",
      "[49] Train loss: 0.010 | Train accuracy: 100.00%\n",
      "[50] Train loss: 0.002 | Train accuracy: 100.00%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for data in train_loader:\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    cost = running_loss / len(train_loader)\n",
    "    \n",
    "    print('[%d] Train loss: %.3f | Train accuracy: %.2f%%' %(epoch + 1, cost, 100.*correct/len(train_loader.dataset))) \n",
    "    \n",
    "    #test loss 값을 Y축, 전달받은 파라미터 epoch를 X 값으로 \n",
    "    vis.line(Y=[cost], X=np.array([epoch]),win=plot,update='append')\n",
    "\n",
    "    # accuracy를 구하는 수식을 Y값으로 epoch를 X값으로 \n",
    "    vis2.line(Y=[100.*correct/len(train_loader.dataset)], X=np.array([epoch]),win=plot2,update='append')\n",
    "    \n",
    "torch.save(model.state_dict(), './models/test4_resnet18.pth')      \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sized-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 4) \n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('./models/test4_resnet18.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "harmful-string",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 78.65 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-rouge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
